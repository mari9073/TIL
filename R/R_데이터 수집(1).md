# R  프로그래밍_데이터 수집(1)



## 1. R 패키지

> R을 가지고 할 수 있는 통계, 분석 그리고 시각화와 관련하여 기능을 정의한 함수들의 묶음이다.
>
> R을 설치할 때 함께 설치되는 기본 패키지가 있고 만약 찾는 기능이 없다면 원하는 기능을 처리해주는 패키지를 찾아서 추가로 설치한 후 사용하면 된다.
>
> R 패키지는 CRAN(https://cran.r-project.org/) 사이트에서 모두 검색 가능하고 다운로드 받을 수 있다.

```R
library()   #설치된 패키지 리스트 확인
installed.packages()   #설치된 패키지 리스트(콘솔창에 출력)
search()   #load된 패키지 점검
install.packages("패키지명")   #새로운 R 패키지 설치
remove.packages("패키지명")   #설치된 패키지 삭제
packageVersion("패키지명")   #설치된 패키지의 버전 확인
update.packages("패키지명")   #설치된 패키지 업데이트
library(패키지명)   #설치된 패키지 로드
require(패키지명)   #설치된 패키지 로드
detach("package:패키지명")   #로드된 패키지 언로드
```

```R
excel_data_ex <- read_excel("data/data_ex.xlsx")   #엑셀문서 읽어오는 함수
getwd()
View(excel_data_ex)   #V를 대문자로 써야함
search()
str(excel_data_ex)
```



## 2. 정적 웹 크롤링과 스크래핑

> 웹 스크래핑(web scraping) : 웹 사이트 상에서 원하는 부분에 위치한 정보를 컴퓨터로 하여금 자동으로 추출하여 수집하는 기술

> 웹 크롤링(web crawling) : 자동화 봇(bot)인 웹 크롤러가 정해진 규칙에 따라 복수 개의 웹 페이지를 브라우징 하는 행위

![scrape](https://user-images.githubusercontent.com/69948774/93713846-906f4200-fb99-11ea-9247-2e538e6301ba.JPG)

#### 1) rvest 패키지

- 주요 함수

```R
html_nodes(id, css, xpath)   #매칭되는 모든 요소를 반환 
html_node(id, css, xpath)   #매칭되는 한 요소만 반환
html_text(x)   #찾아온 요소의 텍스트를 추출
html_text(x, trim=TRUE)   #Trim:앞뒤에 있는 공백, 탭 등을 없애라
html_attrs(x)   #속성값(attribute)을 추출
html_attrs(x, name, default="")
html_name()   #attribute의 이름을 추출
html_children()   #해당 요소의 하위 요소를 추출
html_tag()   #tag 이름 추출
```

- example1

```R
library(rvest)   #R스튜디오를 새로 열었을때 새로 load해 줌

url <- "http://unico2013.dothome.co.kr/crawling/tagstyle.html"
text <- read_html(url)   #url에서 html 파일을 읽어와 저장
> text
{html_document}
<html>
[1] <head>\n<meta http-equiv="Content-Type" content="text/html;  ...
[2] <body>\r\n<h1>블럭스타일 태그</h1>\r\n<div style="background-color: ...

nodes <- html_nodes(text, "div")   #태그 선택자; 모든 div태크를 찾아 추출
> nodes
{xml_nodeset (3)}
[1] <div style="background-color:yellow">테스트입니다1</div>
[2] <div>테스트입니다2</div>
[3] <div>테스트입니다3</div>

title <- html_text(nodes)   #찾아온 태그들의 content만 추출
> title
[1] "테스트입니다1" "테스트입니다2" "테스트입니다3"

node1 <- html_nodes(text, "div:nth-of-type(1)")   #첫번째 div를 꺼내라
> node1
{xml_nodeset (1)}
[1] <div style="background-color:yellow">테스트입니다1</div>

> html_text(node1)
[1] "테스트입니다1"
> html_attr(node1, "style")   #원하는 속성의 값을 꺼내라
[1] "background-color:yellow"

node2 <- html_nodes(text, "div:nth-of-type(2)")
> node2
{xml_nodeset (1)}
[1] <div>테스트입니다2</div>

> html_text(node2)
[1] "테스트입니다2"

> html_attr(node2, "style")  #style이라는 속성이 없어서 NA 출력
[1] NA
```

- example2

```R
# 단일 페이지(rvest 패키지 사용)

text<- NULL; title<-NULL; point<-NULL; review<-NULL; page=NULL #이전에 들어있던 값을 없애주기 위해
url<- "http://movie.naver.com/movie/point/af/list.nhn?page=1"
text <- read_html(url,  encoding="CP949")  #크롤링,웹페이지 소스에서 head(meta)태그의 charset 확인(UTF-8이면 생략가능)
> text
{html_document}
<html lang="ko">
[1] <head>\n<meta http-equiv="Content-Type" content="text/html;  ...
[2] <body>\r\n\r\n\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\t\r\n\t\r ...

# 영화제목
nodes <- html_nodes(text, ".movie")
> html_text(nodes)
 [1] "#살아있다"                             
 [2] "길버트 그레이프"                       
 [3] "오! 문희"                              
 ...

# 영화평점
nodes <- html_nodes(text, ".title em")
> html_text(nodes)
[1] "6"  "10" "10" "2"  "10" "8"  "10" "10" "6"  "1" 

# 영화리뷰
nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
nodes <- html_text(nodes, trim=TRUE)   #Trim:앞뒤에 있는 공백, 탭 등을 없애라
> nodes[nchar(nodes) > 0]   #null 다 없애기
[1] "유아인의 연기는 훌륭 하지만 이 배역은 안재홍 같은 배우가 연기했으면 더 어울렸을 듯. 약간 모자란 듯 하면서.."                                                                                                                     
[2] "27년이나 된 영화인데 전혀 촌스럽지 않았다. 레오의 연기는 최고였고 조니뎁의 얼굴도 최고였다.  울었다  너무 감동적이었다. 90년대 미국 시골마을 배경도 이뻤고 그냥 다 좋았다."  

[3] "오랜만에 엄마랑 영화보러 갔는데 엄마가 엄청 좋아하시고 웃으셔서 기쁘네여ㅎㅎ 뭉클하기도 하고 웃기기도 하고 눈물흘리며 넘나 재밌게 봤습니다!! ><" 
...

page <- data.frame(title, point, review)   #에러:review를 안쓴사람이 있어서 갯수가 안맞음
write.csv(page, "movie_reviews.csv")   #데이터 파일의 네임을 가지고 csv파일로 저장

```

- example3

```R
#여러페이지

site<- "http://movie.naver.com/movie/point/af/list.nhn?page="
text <- NULL
movie.review <- NULL
for(i in 1: 100) {
  url <- paste(site, i, sep="")
  text <- read_html(url,  encoding="CP949")
  nodes <- html_nodes(text, ".movie")
  title <- html_text(nodes)
  nodes <- html_nodes(text, ".title em")
  point <- html_text(nodes)
  nodes <- html_nodes(text, xpath="//*[@id='old_content']/table/tbody/tr/td[2]/text()")
  imsi <- html_text(nodes, trim=TRUE)
  review <- imsi[nchar(imsi) > 0] 
  if(length(review) == 10) {    #한페이지에 리뷰가 10개 다 있으면 실행
    page <- data.frame(title, point, review)   #세개다 10개 갯수 맞음
    movie.review <- rbind(movie.review, page)
  } else {
    cat(paste(i," 페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ\n"))
  }
}
> 1  페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ
2  페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ
5  페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ
...
98  페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ
100  페이지에는 리뷰글이 생략된 데이터가 있어서 수집하지 않습니다.ㅜㅜ

write.csv(movie.review, "movie_reviews2.csv")   #리뷰 10개 있는 페이지들만 저장
```



#### 2) xml 패키지

- 주요 함수

```R
htmlParse(file, encoding="...")   #
xpathSApply(doc, path, fun)   #원하는 항목(xpath)의 자료(fun)를 가져옴
#fun : xmlValue, xmlGetAttr, xmlAttrs
gsub("[[:정규표현식:]]", "", content)
```

- example

```R
#(xml 패키지 사용)
library(XML)
library(rvest)
imsi <- read_html("http://www.hani.co.kr/")
t <- htmlParse(imsi)   #도큐먼트 객체를 리턴받음
content<- xpathSApply(t,'//*[@id="main-top01-scroll-in"]/div/div/h4/a', xmlValue); 
content
content <- gsub("[[:punct:]]", "", content)  #[[정규표현식]]/'특수문자'를 '없음'으로 대체
content
```



#### 3) httr 패키지

- example1

```R
# GET 방식 요청
library(httr)
http.standard <- GET('http://www.w3.org/Protocols/rfc2616/rfc2616.html') #아무정보도 요청하지 않을때는 굳이 get함수 쓰지 x
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
title2 = html_text(title2)
> title2
[1] "Table of Contents"
```

- example2

```R
# POST 방식 요청
library(httr)
otp_url= 'http://marketdata.krx.co.kr/contents/COM/GenerateOTP.jspx'
parameter = list(
  name = 'fileDown',
  filetype = 'csv',
  url = 'MKD/03/0303/03030103/mkd03030103',
  tp_cd = 'ALL',
  date = '20190607',
  lang = 'ko',
  pagePath = '/contents/MKD/03/0303/03030103/MKD03030103.jsp')

my_otp = POST(otp_url, query = parameter) 

download_url = 'http://file.krx.co.kr/download.jspx'
data = POST(download_url, query = list(code = my_otp),
            add_headers(referer = otp_url)) 

library(readr)
data =  read_html(data) 
data = html_text(data)
data = read_csv(data)
as.data.frame(data)
```



#### 4) 그 외

- example1

```R
# 뉴스, 게시판 등 글 목록에서 글의 URL만 뽑아내기 
res = GET('https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001')
htxt = read_html(res)
link = html_nodes(htxt, 'div.list_body a'); length(link)
article.href = unique(html_attr(link, 'href'))
article.href
```

- example2

```R
# 이미지, 첨부파일 다운 받기 

# pdf
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'c:/Temp/httr.pdf')   #다음 경로에 저장

# jpg
h = read_html('http://unico2013.dothome.co.kr/productlog.html')
imgs = html_nodes(h, 'img')
img.src = html_attr(imgs, 'src')
for(i in 1:length(img.src)){    #이미지 하나하나 내려받음
  res = GET(paste('http://unico2013.dothome.co.kr/',img.src[i], sep=""))
  writeBin(content(res, 'raw'), paste('c:/Temp/', img.src[i], sep=""))
} 
```



#### @ 정규 표현식

```R
word <- "JAVA javascript Aa 가나다 AAaAaA123 %^&*"
gsub(" ", "@", word); sub(" ", "@", word)
gsub("A", "", word) 
gsub("a", "", word) 
gsub("Aa", "", word)  #대문자와 소문자가 같이 나온것만 없어짐
gsub("(Aa)", "", word) 
gsub("(Aa){2}", "", word);gsub("Aa{2}", "", word) 
# "JAVA javascript Aa 가나다 AAaAaA123 %^&*"
gsub("[Aa]", "", word)   #[] : or / A or a 다 없애라
gsub("[가-힣]", "", word) 
gsub("[^가-힣]", "", word) 
gsub("[&^%*]", "", word) 
gsub("[[:punct:]]", "", word) 
gsub("[[:alnum:]]", "", word) 
gsub("[1234567890]", "", word) 
gsub("[0-9]", "", word) 
gsub("\\d", "", word); gsub("\\D", "", word)
gsub("[[:digit:]]", "", word) 
gsub("[^[:alnum:]]", "", word) 
gsub("[[:space:]]", "", word) 
gsub("[[:punct:][:digit:]]", "", word)
```



## 3. Open API를 활용한 공공DB와 SNS 데이터 수집

> 함수, 클래스...: API(Aplication Programming Interface)

> URL 문자열 : 필요한 데이터 요청하고 받아갈 수 있게 지원하는 URL 문자열
>
> > 인증키, Query 문자열 등을 필요로 한다.
>
> > 요청 헤더에 규격정보를 제공해야 할수도 있다.

![API](https://user-images.githubusercontent.com/69948774/93713851-9402c900-fb99-11ea-9470-34cabd8dfbcd.png style="zoom: 67%;")
